{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b36c159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hf_xet\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl.metadata (5.0 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.9/2.9 MB 28.1 MB/s  0:00:00\n",
      "Installing collected packages: hf_xet\n",
      "Successfully installed hf_xet-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!\"C:\\Users\\Lokeshwar Reddy\\AppData\\Local\\Programs\\Python\\Python310\\python.exe\" -m pip install hf_xet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ebf8771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datasets loaded successfully!\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tags', 'tokens'],\n",
      "        num_rows: 5228\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tags', 'tokens'],\n",
      "        num_rows: 5330\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tags', 'tokens'],\n",
      "        num_rows: 5865\n",
      "    })\n",
      "})\n",
      "\n",
      "‚úÖ Label mappings created successfully!\n",
      "Number of labels: 5\n",
      "Label List: {'O': 0, 'B-Chemical': 1, 'B-Disease': 2, 'I-Disease': 3, 'I-Chemical': 4}\n",
      "id2label mapping: {'0': 'O', '1': 'B-Chemical', '2': 'B-Disease', '3': 'I-Disease', '4': 'I-Chemical'}\n",
      "label2id mapping: {'O': '0', 'B-Chemical': '1', 'B-Disease': '2', 'I-Disease': '3', 'I-Chemical': '4'}\n",
      "\n",
      "--- Inspecting Training Data (First Example) ---\n",
      "Features: {'tags': List(Value('int64')), 'tokens': List(Value('string'))}\n",
      "{'tags': [1, 0, 0, 0, 0, 0, 1, 0], 'tokens': ['Naloxone', 'reverses', 'the', 'antihypertensive', 'effect', 'of', 'clonidine', '.']}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# --- 1. Define File Paths ---\n",
    "# (Using the paths you provided)\n",
    "data_files = {\n",
    "    'train': 'dataset/train.json',\n",
    "    'test': 'dataset/test.json',\n",
    "    'validation': 'dataset/valid.json'\n",
    "}\n",
    "\n",
    "label_file_path = 'dataset/label.json'\n",
    "\n",
    "# --- 2. Load the Datasets from JSON files ---\n",
    "# We specify 'json' and point to our file paths\n",
    "# The 'load_dataset' function will create a DatasetDict\n",
    "try:\n",
    "    raw_datasets = load_dataset('json', data_files=data_files)\n",
    "    print(\"‚úÖ Datasets loaded successfully!\")\n",
    "    print(raw_datasets)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error loading dataset files: {e}\")\n",
    "    print(\"Please make sure your file paths are correct.\")\n",
    "    # Exit or raise error if you want to stop execution\n",
    "    \n",
    "\n",
    "# --- 3. Load Label Mappings ---\n",
    "# We load the label.json file to create our mappings\n",
    "try:\n",
    "    with open(label_file_path, 'r') as f:\n",
    "        label_list = json.load(f)\n",
    "    \n",
    "    # Create the mappings\n",
    "    id2label = {str(i): label for i, label in enumerate(label_list)}\n",
    "    label2id = {label: str(i) for i, label in enumerate(label_list)}\n",
    "    \n",
    "    num_labels = len(label_list)\n",
    "\n",
    "    print(\"\\n‚úÖ Label mappings created successfully!\")\n",
    "    print(f\"Number of labels: {num_labels}\")\n",
    "    print(f\"Label List: {label_list}\")\n",
    "    print(f\"id2label mapping: {id2label}\")\n",
    "    print(f\"label2id mapping: {label2id}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error loading label file: {e}\")\n",
    "\n",
    "\n",
    "# --- 4. Inspect the Data (Optional but Recommended) ---\n",
    "if 'raw_datasets' in locals():\n",
    "    print(\"\\n--- Inspecting Training Data (First Example) ---\")\n",
    "    \n",
    "    # Check the features of the dataset\n",
    "    print(f\"Features: {raw_datasets['train'].features}\")\n",
    "    \n",
    "    # Print the first example\n",
    "    # This helps you see the 'tokens' and 'tags' (NER labels)\n",
    "    print(raw_datasets['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8f361fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading BioBERT tokenizer...\n",
      "‚úÖ Tokenizer loaded!\n",
      "\n",
      "‚è≥ Starting token-label alignment...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9e02d2b449426dbcdea9eb84111758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer (num_proc=1): 100%|##########| 5228/5228 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe841f046e9446e9efb797177c41afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer (num_proc=1): 100%|##########| 5330/5330 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f3e9e507834107b9f9e56588428cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer (num_proc=1): 100%|##########| 5865/5865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done! Data is ready for BioBERT.\n",
      "[-100, 1, 4, 4, 4, 0, 0, 0, 0, 0]\n",
      "Checking dataset structure...\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tags', 'tokens', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 5228\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tags', 'tokens', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 5330\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tags', 'tokens', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 5865\n",
      "    })\n",
      "})\n",
      "\n",
      "‚úÖ SUCCESS: Dataset has been tokenized and is ready for training!\n",
      "Features: dict_keys(['tags', 'tokens', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
      "‚úÖ Seqeval metric loaded!\n",
      "\n",
      "‚è≥ Loading BioBERT for Token Classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Lokeshwar Reddy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lokeshwar Reddy\\AppData\\Local\\Temp\\ipykernel_30712\\2703805408.py:152: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded!\n",
      "\n",
      "‚úÖ Trainer initialized! Ready to train.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- 1. Setup Mappings & Tokenizer ---\n",
    "# These are defined in the main process\n",
    "ID2LABEL = {'0': 'O', '1': 'B-Chemical', '2': 'B-Disease', '3': 'I-Disease', '4': 'I-Chemical'}\n",
    "LABEL2ID = {'O': '0', 'B-Chemical': '1', 'B-Disease': '2', 'I-Disease': '3', 'I-Chemical': '4'}\n",
    "model_checkpoint = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "\n",
    "print(\"‚è≥ Loading BioBERT tokenizer...\")\n",
    "MAIN_TOKENIZER = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "print(\"‚úÖ Tokenizer loaded!\")\n",
    "\n",
    "# --- 2. Define the Alignment Function (UPDATED to accept ALL needed variables) ---\n",
    "def tokenize_and_align_labels(examples, tokenizer, id2label_map, label2id_map):\n",
    "    # Use passed arguments, not global variables\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], \n",
    "        truncation=True, \n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label_ids in enumerate(examples[\"tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        new_label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                new_label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                new_label_ids.append(label_ids[word_idx])\n",
    "            else:\n",
    "                original_tag_id = label_ids[word_idx]\n",
    "                # Use passed id2label_map\n",
    "                label_name = id2label_map.get(str(original_tag_id))\n",
    "                if label_name and label_name.startswith('B-'):\n",
    "                    i_label_name = 'I-' + label_name[2:]\n",
    "                    # Use passed label2id_map\n",
    "                    if i_label_name in label2id_map:\n",
    "                        new_label_ids.append(int(label2id_map[i_label_name]))\n",
    "                    else:\n",
    "                        new_label_ids.append(original_tag_id)\n",
    "                else:\n",
    "                    new_label_ids.append(original_tag_id)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(new_label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# --- 3. Run Processing (Safe Mode with ALL variables passed) ---\n",
    "if 'raw_datasets' in locals():\n",
    "    print(\"\\n‚è≥ Starting token-label alignment...\")\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_and_align_labels, \n",
    "        batched=True, \n",
    "        num_proc=1, \n",
    "        load_from_cache_file=False,\n",
    "        desc=\"Running tokenizer\",\n",
    "        # CRITICAL FIX: Pass ALL three variables explicitly\n",
    "        fn_kwargs={\n",
    "            \"tokenizer\": MAIN_TOKENIZER,\n",
    "            \"id2label_map\": ID2LABEL,\n",
    "            \"label2id_map\": LABEL2ID\n",
    "        }\n",
    "    )\n",
    "    print(\"‚úÖ Done! Data is ready for BioBERT.\")\n",
    "    \n",
    "    # Optional: Quick check of one example\n",
    "    print(tokenized_datasets['train'][0]['labels'][:10])\n",
    "else:\n",
    "    print(\"‚ùå Error: 'raw_datasets' is missing. Please re-run Phase 1 data loading.\")\n",
    "# --- FINAL VERIFICATION ---\n",
    "print(\"Checking dataset structure...\")\n",
    "print(tokenized_datasets)\n",
    "\n",
    "# Check if 'input_ids' exists in the train features\n",
    "if 'input_ids' in tokenized_datasets['train'].features:\n",
    "    print(\"\\n‚úÖ SUCCESS: Dataset has been tokenized and is ready for training!\")\n",
    "    print(\"Features:\", tokenized_datasets['train'].features.keys())\n",
    "else:\n",
    "    print(\"\\n‚ùå SOMETHING FAILED: 'input_ids' column is missing.\")\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "# --- 1. Load Evaluation Metric ---\n",
    "# We use seqeval, the standard for NER tasks\n",
    "try:\n",
    "    metric = evaluate.load(\"seqeval\")\n",
    "    print(\"‚úÖ Seqeval metric loaded!\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è could not load 'evaluate', trying 'datasets' fallback...\")\n",
    "    from datasets import load_metric\n",
    "    metric = load_metric(\"seqeval\")\n",
    "\n",
    "# --- 2. Define Compute Metrics Function ---\n",
    "# This function handles the -100 ignore index during evaluation\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [ID2LABEL[str(p)] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [ID2LABEL[str(l)] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# --- 3. Load the Model ---\n",
    "print(\"\\n‚è≥ Loading BioBERT for Token Classification...\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    num_labels=len(ID2LABEL),\n",
    "    id2label=ID2LABEL,\n",
    "    label2id=LABEL2ID\n",
    ")\n",
    "print(\"‚úÖ Model loaded!\")\n",
    "\n",
    "# --- 4. Define Training Arguments ---\n",
    "args = TrainingArguments(\n",
    "    \"biobert-finetuned-ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,             # Start with 3 epochs for a quick first run\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",          # Save model after every epoch\n",
    "    load_best_model_at_end=True,    # Load the best model when finished\n",
    "    metric_for_best_model=\"f1\",     # Use F1 score to determine \"best\"\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# --- 5. Initialize the Trainer ---\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=MAIN_TOKENIZER)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=MAIN_TOKENIZER,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Trainer initialized! Ready to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82ccc59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BioBERT tokenizer loaded successfully!\n",
      "\n",
      "--- Applying NEW token-label alignment (B-I propagation) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd239f7f59f4eda84fc2ce1277fad8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5228 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42441e74879e4155b65f457ffa962689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5330 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d374398ec9469cbaf6fbe20fffe891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Token-label alignment complete!\n",
      "\n",
      "--- Processed Dataset ---\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tags', 'tokens', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 5228\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tags', 'tokens', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 5330\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tags', 'tokens', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 5865\n",
      "    })\n",
      "})\n",
      "\n",
      "--- Inspecting First Processed Example ---\n",
      "[CLS]           -100  IGNORE_INDEX\n",
      "na              1     B-Chemical\n",
      "##lo            4     I-Chemical\n",
      "##xon           4     I-Chemical\n",
      "##e             4     I-Chemical\n",
      "reverse         0     O\n",
      "##s             0     O\n",
      "the             0     O\n",
      "anti            0     O\n",
      "##hy            0     O\n",
      "##pert          0     O\n",
      "##ens           0     O\n",
      "##ive           0     O\n",
      "effect          0     O\n",
      "of              0     O\n",
      "c               1     B-Chemical\n",
      "##lon           4     I-Chemical\n",
      "##id            4     I-Chemical\n",
      "##ine           4     I-Chemical\n",
      ".               0     O\n",
      "[SEP]           -100  IGNORE_INDEX\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- 1. Define Mappings (from your Phase 1 output) ---\n",
    "# We need these to convert between tag IDs and names\n",
    "# Note: Based on your output, the IDs are integers, but the\n",
    "# map keys/values are strings. We will handle this.\n",
    "\n",
    "id2label = {'0': 'O', '1': 'B-Chemical', '2': 'B-Disease', '3': 'I-Disease', '4': 'I-Chemical'}\n",
    "label2id = {'O': '0', 'B-Chemical': '1', 'B-Disease': '2', 'I-Disease': '3', 'I-Chemical': '4'}\n",
    "\n",
    "# --- 2. Define Model Checkpoint & Load Tokenizer ---\n",
    "model_checkpoint = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    print(\"‚úÖ BioBERT tokenizer loaded successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading tokenizer: {e}\")\n",
    "\n",
    "\n",
    "# --- 3. Define the NEW Token-Label Alignment Function ---\n",
    "# This function implements the B-I propagation logic\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], \n",
    "        truncation=True, \n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label_ids in enumerate(examples[\"tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        \n",
    "        previous_word_idx = None\n",
    "        new_label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            # Special token (e.g., [CLS], [SEP])\n",
    "            if word_idx is None:\n",
    "                new_label_ids.append(-100) # Still ignore special tokens\n",
    "            \n",
    "            # New word\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # Get the original tag ID\n",
    "                tag_id = label_ids[word_idx]\n",
    "                new_label_ids.append(tag_id)\n",
    "            \n",
    "            # Subsequent subword (same word)\n",
    "            else:\n",
    "                # Get the original tag ID for this word\n",
    "                original_tag_id = label_ids[word_idx]\n",
    "                \n",
    "                # Look up the string name (e.g., 'B-Chemical')\n",
    "                # We convert the int tag_id to a string for the lookup\n",
    "                label_name = id2label.get(str(original_tag_id))\n",
    "\n",
    "                if label_name and label_name.startswith('B-'):\n",
    "                    # It's a \"B-\" tag. Create the corresponding \"I-\" tag\n",
    "                    # e.g., 'B-Chemical' -> 'I-Chemical'\n",
    "                    i_label_name = 'I-' + label_name[2:]\n",
    "                    \n",
    "                    if i_label_name in label2id:\n",
    "                        # Find the ID for the \"I-\" tag (e.g., 4)\n",
    "                        i_label_id = int(label2id[i_label_name])\n",
    "                        new_label_ids.append(i_label_id)\n",
    "                    else:\n",
    "                        # This shouldn't happen if your labels are consistent\n",
    "                        new_label_ids.append(original_tag_id)\n",
    "                else:\n",
    "                    # It was already an 'O' or 'I-' tag, so just repeat it\n",
    "                    new_label_ids.append(original_tag_id)\n",
    "                \n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(new_label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# --- 4. Apply the Function to the Entire Dataset ---\n",
    "# We assume 'raw_datasets' exists from Phase 1\n",
    "if 'raw_datasets' in locals():\n",
    "    print(\"\\n--- Applying NEW token-label alignment (B-I propagation) ---\")\n",
    "    \n",
    "    # Use .map() to apply the function to all splits\n",
    "    tokenized_datasets = raw_datasets.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "    print(\"‚úÖ Token-label alignment complete!\")\n",
    "    print(\"\\n--- Processed Dataset ---\")\n",
    "    print(tokenized_datasets)\n",
    "    \n",
    "    # --- 5. Inspect the Result (Recommended) ---\n",
    "    print(\"\\n--- Inspecting First Processed Example ---\")\n",
    "    example = tokenized_datasets['train'][0]\n",
    "    \n",
    "    # Print subwords and their corresponding new labels\n",
    "    for token, label_id in zip(tokenizer.convert_ids_to_tokens(example[\"input_ids\"]), example[\"labels\"]):\n",
    "        # We'll use id2label to make it readable\n",
    "        # -100 won't be in id2label, so we handle it\n",
    "        label_str = id2label.get(str(label_id), \"IGNORE_INDEX\")\n",
    "        print(f\"{token:<15} {label_id:<5} {label_str}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå 'raw_datasets' not found. Please run Phase 1 code first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c361b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "import evaluate\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- 1. Setup Mappings & Tokenizer ---\n",
    "# These are defined in the main process\n",
    "ID2LABEL = {'0': 'O', '1': 'B-Chemical', '2': 'B-Disease', '3': 'I-Disease', '4': 'I-Chemical'}\n",
    "LABEL2ID = {'O': '0', 'B-Chemical': '1', 'B-Disease': '2', 'I-Disease': '3', 'I-Chemical': '4'}\n",
    "model_checkpoint = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "\n",
    "print(\"‚è≥ Loading BioBERT tokenizer...\")\n",
    "MAIN_TOKENIZER = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "print(\"‚úÖ Tokenizer loaded!\")\n",
    "\n",
    "# --- 2. Define the Alignment Function (UPDATED to accept ALL needed variables) ---\n",
    "def tokenize_and_align_labels(examples, tokenizer, id2label_map, label2id_map):\n",
    "    # Use passed arguments, not global variables\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], \n",
    "        truncation=True, \n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label_ids in enumerate(examples[\"tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        new_label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                new_label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                new_label_ids.append(label_ids[word_idx])\n",
    "            else:\n",
    "                original_tag_id = label_ids[word_idx]\n",
    "                # Use passed id2label_map\n",
    "                label_name = id2label_map.get(str(original_tag_id))\n",
    "                if label_name and label_name.startswith('B-'):\n",
    "                    i_label_name = 'I-' + label_name[2:]\n",
    "                    # Use passed label2id_map\n",
    "                    if i_label_name in label2id_map:\n",
    "                        new_label_ids.append(int(label2id_map[i_label_name]))\n",
    "                    else:\n",
    "                        new_label_ids.append(original_tag_id)\n",
    "                else:\n",
    "                    new_label_ids.append(original_tag_id)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(new_label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# --- 3. Run Processing (Safe Mode with ALL variables passed) ---\n",
    "if 'raw_datasets' in locals():\n",
    "    print(\"\\n‚è≥ Starting token-label alignment...\")\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_and_align_labels, \n",
    "        batched=True, \n",
    "        num_proc=1, \n",
    "        load_from_cache_file=False,\n",
    "        desc=\"Running tokenizer\",\n",
    "        # CRITICAL FIX: Pass ALL three variables explicitly\n",
    "        fn_kwargs={\n",
    "            \"tokenizer\": MAIN_TOKENIZER,\n",
    "            \"id2label_map\": ID2LABEL,\n",
    "            \"label2id_map\": LABEL2ID\n",
    "        }\n",
    "    )\n",
    "    print(\"‚úÖ Done! Data is ready for BioBERT.\")\n",
    "    \n",
    "    # Optional: Quick check of one example\n",
    "    print(tokenized_datasets['train'][0]['labels'][:10])\n",
    "else:\n",
    "    print(\"‚ùå Error: 'raw_datasets' is missing. Please re-run Phase 1 data loading.\")\n",
    "\n",
    "\n",
    "# We will use the 'seqeval' metric, as specified in the project plan\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "# --- 1. Load the Model ---\n",
    "# We assume 'model_checkpoint', 'id2label', and 'label2id' exist\n",
    "# from the previous steps.\n",
    "try:\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_checkpoint, \n",
    "        num_labels=num_labels, # This was '5' in your Phase 1\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "    print(\"‚úÖ BioBERT model (AutoModelForTokenClassification) loaded successfully!\")\n",
    "    \n",
    "except NameError as e:\n",
    "    print(f\"‚ùå Error: A variable from Phase 1 is missing (e.g., 'model_checkpoint' or 'id2label'). {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "\n",
    "\n",
    "# --- 2. Define the Data Collator ---\n",
    "# This will pad our inputs and labels dynamically\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "print(\"‚úÖ Data collator initialized.\")\n",
    "\n",
    "\n",
    "# --- 3. Define the Metrics Calculation Function ---\n",
    "# This function will be called at the end of each epoch to compute F1, etc.\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    \n",
    "    # Get the most likely prediction (argmax)\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Convert numeric labels back to string labels\n",
    "    # We need to remove the -100 \"IGNORE_INDEX\" labels\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # Get the results from seqeval\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    # As per your plan, we want the overall scores\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ 'compute_metrics' function defined.\")\n",
    "\n",
    "\n",
    "# --- 4. Set Up Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/biobert\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,           # You can adjust this (3 is a good start)\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",        # Save checkpoint every epoch\n",
    "    load_best_model_at_end=True,  # Load the best model based on loss\n",
    "    push_to_hub=False,            # Set to True if you want to upload\n",
    ")\n",
    "print(\"‚úÖ TrainingArguments defined.\")\n",
    "\n",
    "\n",
    "# --- 5. Initialize the Trainer ---\n",
    "if 'model' in locals():\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"], # Use 'validation' split\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    print(\"\\nüéâ Trainer is initialized and ready to go!\")\n",
    "    print(\"You can now start training by running: trainer.train()\")\n",
    "else:\n",
    "    print(\"‚ùå Model was not loaded. Trainer cannot be initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# --- 1. Global Configuration ---\n",
    "# These paths match exactly what you provided earlier\n",
    "DATA_FILES = {\n",
    "    'train': 'dataset/train.json',\n",
    "    'validation': 'dataset/valid.json',\n",
    "    'test': 'dataset/test.json'\n",
    "}\n",
    "LABEL_FILE = 'dataset/label.json'\n",
    "\n",
    "# --- 2. Load Raw Data ---\n",
    "print(\"‚è≥ Loading dataset from local files...\")\n",
    "raw_datasets = load_dataset('json', data_files=DATA_FILES)\n",
    "print(\"‚úÖ Raw dataset loaded successfully!\")\n",
    "\n",
    "# --- 3. Load & Define Master Label Mappings ---\n",
    "with open(LABEL_FILE, 'r') as f:\n",
    "    label_list = json.load(f)\n",
    "\n",
    "# Create global mappings used by ALL tracks\n",
    "ID2LABEL = {str(i): label for i, label in enumerate(label_list)}\n",
    "LABEL2ID = {label: str(i) for i, label in enumerate(label_list)}\n",
    "\n",
    "print(\"\\n‚úÖ Master label mappings established:\")\n",
    "print(f\"Total Labels: {len(label_list)}\")\n",
    "print(f\"ID2LABEL: {ID2LABEL}\")\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- 1. Track A Configuration ---\n",
    "MODEL_CHECKPOINT = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "\n",
    "print(f\"‚è≥ Loading tokenizer for {MODEL_CHECKPOINT}...\")\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "print(\"‚úÖ BioBERT Tokenizer loaded.\")\n",
    "\n",
    "# --- 2. The Robust Alignment Function ---\n",
    "# This function fixes the \"subword\" problem by propagating labels correctly.\n",
    "def align_labels_biobert(examples, tokenizer_obj, id2label_map, label2id_map):\n",
    "    tokenized_inputs = tokenizer_obj(\n",
    "        examples[\"tokens\"], \n",
    "        truncation=True, \n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label_ids in enumerate(examples[\"tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        new_label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # 1. Handle Special Tokens ([CLS], [SEP]) -> Ignore (-100)\n",
    "            if word_idx is None:\n",
    "                new_label_ids.append(-100)\n",
    "            # 2. Handle New Words -> Keep original label\n",
    "            elif word_idx != previous_word_idx:\n",
    "                new_label_ids.append(label_ids[word_idx])\n",
    "            # 3. Handle Subsequent Subwords -> Propagate \"I-\" tag\n",
    "            else:\n",
    "                original_tag_id = label_ids[word_idx]\n",
    "                label_name = id2label_map.get(str(original_tag_id))\n",
    "                # If it was a \"B-\" tag, change it to \"I-\" for subwords\n",
    "                if label_name and label_name.startswith('B-'):\n",
    "                    i_label_name = 'I-' + label_name[2:]\n",
    "                    # Look up new ID, or keep original if \"I-\" version doesn't exist\n",
    "                    new_label_ids.append(int(label2id_map.get(i_label_name, original_tag_id)))\n",
    "                else:\n",
    "                    # Otherwise just keep the same tag (O stays O, I- stays I-)\n",
    "                    new_label_ids.append(original_tag_id)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(new_label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# --- 3. Execute Preprocessing ---\n",
    "print(\"\\n‚è≥ Starting Token-Label Alignment (Track A)...\")\n",
    "# We pass all variables explicitly to avoid multiprocessing errors\n",
    "tokenized_datasets_A = raw_datasets.map(\n",
    "    align_labels_biobert,\n",
    "    batched=True,\n",
    "    num_proc=1, # Keep at 1 for safety in notebooks\n",
    "    load_from_cache_file=False, # Ensure fresh run\n",
    "    desc=\"Aligning labels for BioBERT\",\n",
    "    fn_kwargs={\"tokenizer_obj\": TOKENIZER, \"id2label_map\": ID2LABEL, \"label2id_map\": LABEL2ID}\n",
    ")\n",
    "print(\"‚úÖ Track A Data Ready!\")\n",
    "\n",
    "# --- 4. Verification ---\n",
    "# Always check the first example to ensure alignment worked as expected\n",
    "print(\"\\nüìä Verification (First 15 tokens of train[0]):\")\n",
    "ex = tokenized_datasets_A['train'][0]\n",
    "for token, label in zip(TOKENIZER.convert_ids_to_tokens(ex['input_ids'][:15]), ex['labels'][:15]):\n",
    "    print(f\"{token:<12} | {label:<4} ({ID2LABEL.get(str(label), 'IGNORE')})\")\n",
    "\n",
    "\n",
    "print(\"üìä Final Verification (Last 5 tokens):\")\n",
    "ex = tokenized_datasets_A['train'][0]\n",
    "for token, label in zip(TOKENIZER.convert_ids_to_tokens(ex['input_ids'][-5:]), ex['labels'][-5:]):\n",
    "    print(f\"{token:<12} | {label:<4} ({ID2LABEL.get(str(label), 'IGNORE')})\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# --- 0. GPU Check ---\n",
    "# This tells you if PyTorch can actually SEE your GPU.\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"üöÄ Training will automatically use the GPU.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU NOT detected. Training will run SLOWLY on CPU.\")\n",
    "    print(\"If you have a GPU, ensure you installed the CUDA version of PyTorch.\")\n",
    "\n",
    "# --- 1. Metrics Setup ---\n",
    "METRIC = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics_biobert(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Filter out the -100 ignore tokens\n",
    "    true_predictions = [\n",
    "        [ID2LABEL[str(p)] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [ID2LABEL[str(l)] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = METRIC.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# --- 2. Model Loading (with ERROR FIX) ---\n",
    "print(f\"\\n‚è≥ Loading BioBERT Model: {MODEL_CHECKPOINT}...\")\n",
    "model_A = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    num_labels=len(ID2LABEL),\n",
    "    id2label=ID2LABEL,\n",
    "    label2id=LABEL2ID,\n",
    "    # CRITICAL FIX for older PyTorch versions:\n",
    "    weights_only=False \n",
    ")\n",
    "print(\"‚úÖ Model loaded successfully.\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"biobert_track_a_improved_output\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,             # Increased epochs to 10\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    warmup_ratio=0.1,                # Add warmup for learning rate\n",
    "    lr_scheduler_type=\"linear\",      # Use linear decay for learning rate\n",
    ")\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=TOKENIZER)\n",
    "# --- Initialize Trainer with Early Stopping ---\n",
    "trainer_A = Trainer(\n",
    "    model=model_A,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets_A[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_A[\"validation\"],\n",
    "    tokenizer=TOKENIZER,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics_biobert,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # Stop after 3 epochs of no improvement\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Improved Track A (BioBERT) Trainer is ready! Run 'trainer_A.train()' to begin.\")\n",
    "\n",
    "\n",
    "trainer_A.train()\n",
    "\n",
    "# --- Save Track A (BioBERT) Model ---\n",
    "# This is CRITICAL to avoid re-training later.\n",
    "save_path_a = \"./saved_models/biobert_track_a\"\n",
    "trainer_A.save_model(save_path_a)\n",
    "TOKENIZER.save_pretrained(save_path_a)\n",
    "\n",
    "print(f\"‚úÖ Track A (BioBERT) model successfully saved to: {save_path_a}\")\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, EarlyStoppingCallback, DataCollatorForTokenClassification\n",
    "import torch\n",
    "# --- 1. Track B Configuration ---\n",
    "MODEL_CHECKPOINT_B = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "print(f\"‚è≥ Loading DeBERTa tokenizer: {MODEL_CHECKPOINT_B}...\")\n",
    "TOKENIZER_B = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT_B)\n",
    "print(\"‚úÖ DeBERTa Tokenizer loaded.\")\n",
    "\n",
    "# --- 2. Data Processing for DeBERTa ---\n",
    "# We reuse the EXACT SAME alignment function 'align_labels_biobert' from Track A\n",
    "# but we pass it the NEW tokenizer. This ensures perfectly fair comparison.\n",
    "print(\"\\n‚è≥ Aligning data for DeBERTa (Track B)...\")\n",
    "tokenized_datasets_B = raw_datasets.map(\n",
    "    align_labels_biobert,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Aligning for DeBERTa\",\n",
    "    fn_kwargs={\"tokenizer_obj\": TOKENIZER_B, \"id2label_map\": ID2LABEL, \"label2id_map\": LABEL2ID}\n",
    ")\n",
    "\n",
    "# --- 3. Load DeBERTa Model ---\n",
    "print(f\"\\n‚è≥ Loading DeBERTa Model: {MODEL_CHECKPOINT_B}...\")\n",
    "model_B = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT_B,\n",
    "    num_labels=len(ID2LABEL),\n",
    "    id2label=ID2LABEL,\n",
    "    label2id=LABEL2ID,\n",
    "    weights_only=False # Safety bypass for older PyTorch\n",
    ")\n",
    "\n",
    "# --- 4. Define Track B Trainer ---\n",
    "# Using comparable settings to Track A for fairness\n",
    "args_B = TrainingArguments(\n",
    "    output_dir=\"deberta_track_b_output\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16, # If you get CUDA out of memory, change this to 8\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer_B = Trainer(\n",
    "    model=model_B,\n",
    "    args=args_B,\n",
    "    train_dataset=tokenized_datasets_B[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_B[\"validation\"],\n",
    "    tokenizer=TOKENIZER_B,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer=TOKENIZER_B),\n",
    "    compute_metrics=compute_metrics_biobert, # We can reuse the same metric function\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "\n",
    "print(\"üöÄ Starting Track B (DeBERTa) training...\")\n",
    "trainer_B.train()\n",
    "\n",
    "# --- Save Track B (DeBERTa) Model ---\n",
    "save_path_b = \"./saved_models/deberta_track_b\"\n",
    "trainer_B.save_model(save_path_b)\n",
    "TOKENIZER_B.save_pretrained(save_path_b)\n",
    "\n",
    "print(f\"‚úÖ Track B (DeBERTa) model successfully saved to: {save_path_b}\")\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# --- 1. Build Vocabulary (FIXED: all lowercase) ---\n",
    "print(\"‚è≥ Building lowercase vocabulary from training data...\")\n",
    "word_counts = Counter()\n",
    "for example in raw_datasets['train']:\n",
    "    # Convert all tokens to lowercase before counting\n",
    "    word_counts.update([w.lower() for w in example['tokens']])\n",
    "\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "# Create mappings\n",
    "word2id = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "for word, _ in word_counts.most_common():\n",
    "    word2id[word] = len(word2id)\n",
    "\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "\n",
    "VOCAB_SIZE = len(word2id)\n",
    "print(f\"‚úÖ Lowercase vocabulary built! Size: {VOCAB_SIZE} unique words.\")\n",
    "print(f\"Example mapping: 'the' -> {word2id.get('the')}\")\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "import torch\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "\n",
    "# --- Re-define Configuration ---\n",
    "# (Make sure these match your actual paths)\n",
    "FASTTEXT_PATH = \"./embeddings/wiki-news-300d-1M.vec\"\n",
    "BIOWORDVEC_PATH = \"./embeddings/bio_embedding_intrinsic\"\n",
    "\n",
    "# --- Superior Loading Function (using Gensim) ---\n",
    "def load_embeddings_gensim(path, word2id, embedding_dim, binary=False):\n",
    "    print(f\"‚è≥ Loading embeddings from {path} (Binary={binary})...\")\n",
    "\n",
    "    # 1. Initialize standard random matrix\n",
    "    vocab_size = len(word2id)\n",
    "    embedding_matrix = np.random.uniform(-0.25, 0.25, (vocab_size, embedding_dim))\n",
    "    embedding_matrix[word2id[\"<PAD>\"]] = np.zeros((embedding_dim,))\n",
    "\n",
    "    try:\n",
    "        # 2. Use Gensim to load the file (fast and robust)\n",
    "        # limit=500000 loads only top 500k words to save RAM, remove if you have >16GB RAM\n",
    "        kv = KeyedVectors.load_word2vec_format(path, binary=binary, limit=500000)\n",
    "\n",
    "        # 3. Transfer weights to our matrix\n",
    "        hit_count = 0\n",
    "        for word, idx in word2id.items():\n",
    "            if word in kv:\n",
    "                embedding_matrix[idx] = kv[word]\n",
    "                hit_count += 1\n",
    "\n",
    "        print(f\"‚úÖ Loaded! Coverage: {hit_count / vocab_size:.2%}\")\n",
    "        return embedding_matrix\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå ERROR: File not found at {path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR loading file: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Execute Loads ---\n",
    "# Ensure 'word2id' exists from previous steps!\n",
    "\n",
    "# 1. Load FastText (Text format, binary=False)\n",
    "print(\"\\n--- Loading Track C2 (FastText) ---\")\n",
    "embedding_matrix_fasttext = load_embeddings_gensim(FASTTEXT_PATH, word2id, 300, binary=False)\n",
    "\n",
    "# 2. Load BioWordVec (Binary format, binary=True)\n",
    "print(\"\\n--- Loading Track C1 (BioWordVec) ---\")\n",
    "# Notice we set binary=True here to fix your error\n",
    "embedding_matrix_bio = load_embeddings_gensim(BIOWORDVEC_PATH, word2id, 200, binary=True)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# --- 1. Define Custom Dataset Class (FIXED: all lowercase) ---\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, word2id, label2id):\n",
    "        self.dataset = hf_dataset\n",
    "        self.word2id = word2id\n",
    "        self.label2id = label2id\n",
    "        self.unk_id = word2id[\"<UNK>\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        # Convert words to lowercase IDs\n",
    "        token_ids = [self.word2id.get(w.lower(), self.unk_id) for w in item['tokens']]\n",
    "        label_ids = item['tags']\n",
    "\n",
    "        return {\n",
    "            'token_ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "            'label_ids': torch.tensor(label_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# --- 2. Define Collate Function (No changes needed) ---\n",
    "def collate_fn(batch):\n",
    "    token_seqs = [item['token_ids'] for item in batch]\n",
    "    label_seqs = [item['label_ids'] for item in batch]\n",
    "    padded_tokens = pad_sequence(token_seqs, batch_first=True, padding_value=0)\n",
    "    padded_labels = pad_sequence(label_seqs, batch_first=True, padding_value=-100)\n",
    "    attention_masks = (padded_tokens != 0).long()\n",
    "    return padded_tokens, padded_labels, attention_masks\n",
    "\n",
    "# --- 3. Create DataLoaders ---\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = NERDataset(raw_datasets['train'], word2id, LABEL2ID)\n",
    "valid_dataset = NERDataset(raw_datasets['validation'], word2id, LABEL2ID)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(\"‚úÖ PyTorch DataLoaders created!\")\n",
    "# Test one batch\n",
    "sample_tokens, sample_labels, sample_masks = next(iter(train_loader))\n",
    "print(f\"Sample batch shape: {sample_tokens.shape}\")\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "# --- 1. Install CRF Library (if needed) ---\n",
    "# If you already ran this, you can comment it out, but it's safe to leave.\n",
    "# !pip install pytorch-crf\n",
    "\n",
    "# --- 2. Define the Model Architecture ---\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_labels, embedding_matrix=None):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        \n",
    "        # 1. Embedding Layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Load pre-trained weights\n",
    "        if embedding_matrix is not None:\n",
    "            print(\"üîß Loading pre-trained embedding weights into model...\")\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        \n",
    "        # 2. BiLSTM Layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, \n",
    "                            num_layers=1, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # 3. Linear Mapping (Hidden -> Tag Space)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, num_labels)\n",
    "        \n",
    "        # 4. CRF Layer\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "        \n",
    "    def forward(self, input_ids, mask):\n",
    "        embeds = self.embedding(input_ids)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        emissions = self.hidden2tag(lstm_out)\n",
    "        return emissions\n",
    "\n",
    "    def compute_loss(self, emissions, tags, mask):\n",
    "        log_likelihood = self.crf(emissions, tags, mask=mask.bool())\n",
    "        return -log_likelihood\n",
    "\n",
    "    def decode(self, emissions, mask):\n",
    "        return self.crf.decode(emissions, mask=mask.bool())\n",
    "\n",
    "print(\"‚úÖ BiLSTM_CRF model class defined!\")\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LABELS = len(LABEL2ID)\n",
    "VOCAB_SIZE = len(word2id)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "\n",
    "# --- Track C1: BioWordVec Model ---\n",
    "print(\"\\nüèóÔ∏è Initializing Track C1 (BioWordVec)...\")\n",
    "model_c1 = BiLSTM_CRF(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=200, # BioWordVec is 200d\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_labels=NUM_LABELS,\n",
    "    embedding_matrix=embedding_matrix_bio\n",
    ")\n",
    "model_c1.to(device)\n",
    "\n",
    "# --- Track C2: FastText Model ---\n",
    "print(\"\\nüèóÔ∏è Initializing Track C2 (FastText)...\")\n",
    "model_c2 = BiLSTM_CRF(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=300, # FastText is 300d\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_labels=NUM_LABELS,\n",
    "    embedding_matrix=embedding_matrix_fasttext\n",
    ")\n",
    "model_c2.to(device)\n",
    "\n",
    "print(\"\\n‚úÖ Both custom models initialized and moved to GPU!\")\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Ensure METRIC is loaded globally from previous steps\n",
    "\n",
    "# --- UPDATED Training Loop with CRF Padding Fix ---\n",
    "def train_bilstm(model, train_loader, valid_loader, epochs=10, learning_rate=0.01, patience=3, name=\"Model\"):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    best_f1 = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    print(f\"\\nüöÄ Starting training for {name}...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # --- TRAINING PHASE ---\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\"):\n",
    "            tokens, labels, mask = [b.to(device) for b in batch]\n",
    "            \n",
    "            # CRITICAL FIX: Replace -100 padding with a valid tag (e.g., 0 for 'O')\n",
    "            # The mask will still correctly tell the CRF to ignore these positions.\n",
    "            safe_labels = torch.where(mask.bool(), labels, torch.tensor(0, device=device))\n",
    "            \n",
    "            model.zero_grad()\n",
    "            emissions = model(tokens, mask)\n",
    "            \n",
    "            # Use safe_labels here instead of raw labels\n",
    "            loss = model.compute_loss(emissions, safe_labels, mask)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # --- VALIDATION PHASE ---\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(valid_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\"):\n",
    "                tokens, labels, mask = [b.to(device) for b in batch]\n",
    "                emissions = model(tokens, mask)\n",
    "                pred_tags = model.decode(emissions, mask)\n",
    "                \n",
    "                for i, sent_preds in enumerate(pred_tags):\n",
    "                    real_len = mask[i].sum().item()\n",
    "                    sent_labels = labels[i][:real_len].cpu().numpy()\n",
    "                    sent_preds = sent_preds[:real_len]\n",
    "                    all_preds.append([ID2LABEL[str(p)] for p in sent_preds])\n",
    "                    all_labels.append([ID2LABEL[str(l)] for l in sent_labels])\n",
    "\n",
    "        # --- METRICS ---\n",
    "        results = METRIC.compute(predictions=all_preds, references=all_labels)\n",
    "        f1 = results[\"overall_f1\"]\n",
    "        print(f\"üìä Epoch {epoch+1}: Train Loss={avg_train_loss:.4f} | Val F1={f1:.4%}\")\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), f\"{name}_best.pth\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"\\nüõë Early stopping triggered at epoch {epoch+1}.\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\nüèÅ Training complete for {name}! Best Val F1: {best_f1:.4%}\")\n",
    "    model.load_state_dict(torch.load(f\"{name}_best.pth\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "# --- Run Track C1 (BioWordVec) ---\n",
    "# 10 epochs, stop if no improvement for 3 epochs\n",
    "model_c1_trained = train_bilstm(model_c1, train_loader, valid_loader, \n",
    "                                epochs=10, patience=3, learning_rate=0.01, name=\"Track_C1_BioWordVec\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Run Track C2 (FastText) ---\n",
    "model_c2_trained = train_bilstm(model_c2, train_loader, valid_loader, \n",
    "                                epochs=10, patience=3, learning_rate=0.01, name=\"Track_C2_FastText\")\n",
    "\n",
    "\n",
    "# --- Create Test Loader for Track C ---\n",
    "# We reuse the same Dataset/collate_fn definitions\n",
    "test_dataset = NERDataset(raw_datasets['test'], word2id, LABEL2ID)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(\"‚úÖ Track C Test DataLoader is ready.\")\n",
    "\n",
    "# --- 1. Define function to add word_ids ---\n",
    "# We need to re-pass the tokenizers to this function\n",
    "def add_word_ids(example, tokenizer_obj):\n",
    "    # This creates the list of word_ids for the tokenized inputs\n",
    "    word_ids = tokenizer_obj(example[\"tokens\"], truncation=True, is_split_into_words=True).word_ids()\n",
    "    return {\"word_ids\": word_ids}\n",
    "\n",
    "print(\"‚è≥ Adding word_ids to Track A (BioBERT) test set...\")\n",
    "tokenized_datasets_A[\"test\"] = tokenized_datasets_A[\"test\"].map(\n",
    "    add_word_ids, \n",
    "    fn_kwargs={\"tokenizer_obj\": TOKENIZER},\n",
    "    num_proc=1\n",
    ")\n",
    "\n",
    "print(\"‚è≥ Adding word_ids to Track B (DeBERTa) test set...\")\n",
    "tokenized_datasets_B[\"test\"] = tokenized_datasets_B[\"test\"].map(\n",
    "    add_word_ids, \n",
    "    fn_kwargs={\"tokenizer_obj\": TOKENIZER_B},\n",
    "    num_proc=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Test sets are now ready for span-level evaluation.\")\n",
    "# Check the new feature\n",
    "print(tokenized_datasets_A[\"test\"].features)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "\n",
    "# --- REVISED Evaluation Function for Trainers (A & B) ---\n",
    "def evaluate_transformer(trainer, test_dataset):\n",
    "    print(f\"\\n‚è≥ Evaluating {trainer.model.name_or_path} (Span-Level)...\")\n",
    "    \n",
    "    # 1. Get raw predictions from the trainer\n",
    "    predictions_output, label_ids, _ = trainer.predict(test_dataset)\n",
    "    predictions = np.argmax(predictions_output, axis=2)\n",
    "\n",
    "    # 2. Get the word_ids from the test_dataset (which we just added)\n",
    "    all_word_ids = test_dataset['word_ids']\n",
    "\n",
    "    reconciled_preds = []\n",
    "    reconciled_labels = []\n",
    "\n",
    "    # 3. Loop through each sentence\n",
    "    for i in range(len(all_word_ids)):\n",
    "        word_ids = all_word_ids[i]\n",
    "        preds = predictions[i]\n",
    "        labels = label_ids[i]\n",
    "        \n",
    "        reconciled_sentence_preds = []\n",
    "        reconciled_sentence_labels = []\n",
    "        previous_word_idx = None\n",
    "        \n",
    "        # 4. Loop through subword tokens\n",
    "        for j, word_idx in enumerate(word_ids):\n",
    "            # Skip special tokens ([CLS], [SEP])\n",
    "            if word_idx is None:\n",
    "                continue\n",
    "                \n",
    "            # If this is a new word (i.e., the first subword)\n",
    "            if word_idx != previous_word_idx:\n",
    "                # This is the \"first subword\" rule:\n",
    "                # We append the label for this subword\n",
    "                reconciled_sentence_preds.append(ID2LABEL[str(preds[j])])\n",
    "                reconciled_sentence_labels.append(ID2LABEL[str(labels[j])])\n",
    "            \n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        # Add the fully reconciled word-level lists\n",
    "        reconciled_preds.append(reconciled_sentence_preds)\n",
    "        reconciled_labels.append(reconciled_sentence_labels)\n",
    "\n",
    "    # 5. Calculate metrics on the WORD-LEVEL span labels\n",
    "    print(\"‚úÖ Span-level reconciliation complete. Final Results:\")\n",
    "    print(classification_report(reconciled_labels, reconciled_preds, digits=4))\n",
    "\n",
    "\n",
    "# --- Evaluation Function for BiLSTM-CRF (FIXED) ---\n",
    "def evaluate_bilstm(model_path, model_instance, test_loader):\n",
    "    print(f\"\\n‚è≥ Evaluating {model_path} on test set...\")\n",
    "    \n",
    "    # Load the best weights\n",
    "    # FIX: Removed 'weights_only=True', which is not a valid arg here\n",
    "    # We load the state_dict *from* the file\n",
    "    model_instance.load_state_dict(torch.load(model_path))\n",
    "    model_instance.to(device)\n",
    "    model_instance.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            tokens, labels, mask = [b.to(device) for b in batch]\n",
    "            emissions = model_instance(tokens, mask)\n",
    "            pred_tags = model_instance.decode(emissions, mask)\n",
    "            \n",
    "            for i, sent_preds in enumerate(pred_tags):\n",
    "                real_len = mask[i].sum().item()\n",
    "                sent_labels = labels[i][:real_len].cpu().numpy()\n",
    "                sent_preds = sent_preds[:real_len]\n",
    "                all_preds.append([ID2LABEL[str(p)] for p in sent_preds])\n",
    "                all_labels.append([ID2LABEL[str(l)] for l in sent_labels])\n",
    "                \n",
    "    print(\"‚úÖ Evaluation complete. Results:\")\n",
    "    print(classification_report(all_labels, all_preds, digits=4))\n",
    "\n",
    "print(\"‚úÖ Fixed `evaluate_bilstm` function is defined.\")\n",
    "\n",
    "\n",
    "# --- Run Final Evaluation (Corrected) ---\n",
    "\n",
    "# --- Run Track A (BioBERT) ---\n",
    "evaluate_transformer(trainer_A, tokenized_datasets_A[\"test\"])\n",
    "\n",
    "# --- Run Track B (DeBERTa) ---\n",
    "evaluate_transformer(trainer_B, tokenized_datasets_B[\"test\"])\n",
    "\n",
    "# --- Run Track C1 (BioWordVec) ---\n",
    "# (This function was already correct, as it works at the word level)\n",
    "evaluate_bilstm(\"Track_C1_BioWordVec_best.pth\", model_c1, test_loader)\n",
    "\n",
    "# --- Run Track C2 (FastText) ---\n",
    "evaluate_bilstm(\"Track_C2_FastText_best.pth\", model_c2, test_loader)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "# --- NEW Evaluation Function (Strategy 2b: Prioritized Vote) ---\n",
    "def evaluate_transformer_prioritized_vote(trainer, test_dataset):\n",
    "    print(f\"\\n‚è≥ Evaluating {trainer.model.name_or_path} (Strategy 2b: Prioritized Vote)...\")\n",
    "    \n",
    "    predictions_output, label_ids, _ = trainer.predict(test_dataset)\n",
    "    predictions = np.argmax(predictions_output, axis=2)\n",
    "    all_word_ids = test_dataset['word_ids']\n",
    "\n",
    "    reconciled_preds = []\n",
    "    reconciled_labels = []\n",
    "\n",
    "    for i in range(len(all_word_ids)):\n",
    "        word_ids = all_word_ids[i]\n",
    "        preds = predictions[i]\n",
    "        labels = label_ids[i]\n",
    "        \n",
    "        reconciled_sentence_preds = []\n",
    "        reconciled_sentence_labels = []\n",
    "        \n",
    "        # --- Prioritized Vote Logic ---\n",
    "        current_word_idx = None\n",
    "        current_word_preds = set() # Use a set to store unique predictions\n",
    "        current_word_true_label = 'O'\n",
    "        \n",
    "        for j, word_idx in enumerate(word_ids):\n",
    "            # A. This subword starts a new word\n",
    "            if word_idx != current_word_idx:\n",
    "                # 1. Vote on the PREVIOUS word (if it exists)\n",
    "                if current_word_idx is not None:\n",
    "                    # --- This is the new logic ---\n",
    "                    winner_label = 'O' # Default to O\n",
    "                    \n",
    "                    # Check for B- tags first\n",
    "                    b_tags = [p for p in current_word_preds if p.startswith('B-')]\n",
    "                    if b_tags:\n",
    "                        winner_label = b_tags[0] # Take the first B- tag\n",
    "                    else:\n",
    "                        # No B- tags, check for I- tags\n",
    "                        i_tags = [p for p in current_word_preds if p.startswith('I-')]\n",
    "                        if i_tags:\n",
    "                            winner_label = i_tags[0] # Take the first I- tag\n",
    "                    \n",
    "                    reconciled_sentence_preds.append(winner_label)\n",
    "                    reconciled_sentence_labels.append(current_word_true_label)\n",
    "                \n",
    "                # 2. Reset for the NEW word\n",
    "                current_word_preds = set()\n",
    "                current_word_idx = word_idx\n",
    "                \n",
    "                if word_idx is not None:\n",
    "                    # This is the \"first subword\"\n",
    "                    current_word_true_label = ID2LABEL[str(labels[j])]\n",
    "                    current_word_preds.add(ID2LABEL[str(preds[j])])\n",
    "                else:\n",
    "                    current_word_true_label = 'O'\n",
    "            \n",
    "            # B. This subword continues the current word\n",
    "            elif word_idx is not None:\n",
    "                current_word_preds.add(ID2LABEL[str(preds[j])])\n",
    "        \n",
    "        # C. Tally the votes for the VERY LAST word\n",
    "        if current_word_idx is not None:\n",
    "            winner_label = 'O'\n",
    "            b_tags = [p for p in current_word_preds if p.startswith('B-')]\n",
    "            if b_tags:\n",
    "                winner_label = b_tags[0]\n",
    "            else:\n",
    "                i_tags = [p for p in current_word_preds if p.startswith('I-')]\n",
    "                if i_tags:\n",
    "                    winner_label = i_tags[0]\n",
    "            \n",
    "            reconciled_sentence_preds.append(winner_label)\n",
    "            reconciled_sentence_labels.append(current_word_true_label)\n",
    "\n",
    "        reconciled_preds.append(reconciled_sentence_preds)\n",
    "        reconciled_labels.append(reconciled_sentence_labels)\n",
    "\n",
    "    print(\"‚úÖ Prioritized vote reconciliation complete. Final Results:\")\n",
    "    print(classification_report(reconciled_labels, reconciled_preds, digits=4))\n",
    "\n",
    "\n",
    "# --- Run Evaluation (Strategy 2b - Prioritized) ---\n",
    "\n",
    "print(\"--- 1. Evaluating Track A (BioBERT) with Prioritized Vote ---\")\n",
    "evaluate_transformer_prioritized_vote(trainer_A, tokenized_datasets_A[\"test\"])\n",
    "\n",
    "print(\"\\n--- 2. Evaluating Track B (DeBERTa) with Prioritized Vote ---\")\n",
    "evaluate_transformer_prioritized_vote(trainer_B, tokenized_datasets_B[\"test\"])\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# --- NEW Trace Function (Strategy 2b: Prioritized Vote) ---\n",
    "def trace_prioritized_vote(trainer, tokenized_test_dataset, raw_test_dataset, indices_to_trace):\n",
    "    \n",
    "    examples_to_trace = tokenized_test_dataset.select(indices_to_trace)\n",
    "    raw_examples_to_trace = raw_test_dataset.select(indices_to_trace)\n",
    "    \n",
    "    predictions_output, label_ids, _ = trainer.predict(examples_to_trace)\n",
    "    predictions = np.argmax(predictions_output, axis=2)\n",
    "    all_word_ids = examples_to_trace['word_ids']\n",
    "    \n",
    "    tokenizer = trainer.tokenizer\n",
    "    \n",
    "    for i, idx in enumerate(indices_to_trace):\n",
    "        print(f\"\\n--- üåä TRACING EXAMPLE {idx} (Prioritized Vote) ---\")\n",
    "        \n",
    "        original_tokens = raw_examples_to_trace[i]['tokens']\n",
    "        subword_ids = examples_to_trace[i]['input_ids']\n",
    "        subwords = tokenizer.convert_ids_to_tokens(subword_ids)\n",
    "        word_ids = all_word_ids[i]\n",
    "        pred_labels_ids = predictions[i]\n",
    "        true_labels_ids = label_ids[i]\n",
    "\n",
    "        print(f\"Original Sentence:\\n{' '.join(original_tokens)}\\n\")\n",
    "        print(\"Reconciliation Trace (Strategy 2b: Prioritized Vote):\")\n",
    "        print(\"-------------------------------------------------------------------------------------\")\n",
    "        print(f\"{'Subword':<15} | {'Pred Label':<12} | {'True Label':<12} | {'Word ID':<5} | {'Vote Status'}\")\n",
    "        print(\"-------------------------------------------------------------------------------------\")\n",
    "\n",
    "        reconciled_preds = []\n",
    "        reconciled_labels = []\n",
    "        \n",
    "        current_word_idx = None\n",
    "        current_word_preds = set() # Use a set for unique predictions\n",
    "        current_word_true_label = 'O'\n",
    "        \n",
    "        for j, word_idx in enumerate(word_ids):\n",
    "            \n",
    "            pred_label_str = ID2LABEL.get(str(pred_labels_ids[j]), \"PAD\")\n",
    "            true_label_str = ID2LABEL.get(str(true_labels_ids[j]), \"PAD\")\n",
    "            \n",
    "            # A. This subword starts a new word\n",
    "            if word_idx != current_word_idx:\n",
    "                \n",
    "                # 1. Vote on the PREVIOUS word (if it exists)\n",
    "                if current_word_idx is not None:\n",
    "                    # --- This is the new logic ---\n",
    "                    winner_label = 'O' # Default to O\n",
    "                    b_tags = [p for p in current_word_preds if p.startswith('B-')]\n",
    "                    if b_tags:\n",
    "                        winner_label = b_tags[0] # Priority 1: B- tags\n",
    "                    else:\n",
    "                        i_tags = [p for p in current_word_preds if p.startswith('I-')]\n",
    "                        if i_tags:\n",
    "                            winner_label = i_tags[0] # Priority 2: I- tags\n",
    "                    \n",
    "                    reconciled_preds.append(winner_label)\n",
    "                    reconciled_labels.append(current_word_true_label)\n",
    "                    \n",
    "                    print(\"-------------------------------------------------------------------------------------\")\n",
    "                    print(f\"TALLY VOTES for Word {current_word_idx}: {current_word_preds} -> WINNER: {winner_label} (B-tag priority)\")\n",
    "                    print(\"-------------------------------------------------------------------------------------\")\n",
    "                \n",
    "                # 2. Reset for the NEW word\n",
    "                current_word_preds = set()\n",
    "                current_word_idx = word_idx\n",
    "                \n",
    "                if word_idx is not None:\n",
    "                    current_word_true_label = ID2LABEL[str(true_labels_ids[j])]\n",
    "                    current_word_preds.add(pred_label_str)\n",
    "                    print(f\"{subwords[j]:<15} | {pred_label_str:<12} | {true_label_str:<12} | {word_idx:<5} | Add 1st vote: {pred_label_str}\")\n",
    "                else:\n",
    "                    current_word_true_label = 'O'\n",
    "                    print(f\"{subwords[j]:<15} | {pred_label_str:<12} | {true_label_str:<12} | {'None':<5} | Skipping [CLS]/[SEP]\")\n",
    "            \n",
    "            # B. This subword continues the current word\n",
    "            elif word_idx is not None:\n",
    "                current_word_preds.add(pred_label_str)\n",
    "                print(f\"{subwords[j]:<15} | {pred_label_str:<12} | {true_label_str:<12} | {word_idx:<5} | Add vote: {pred_label_str}\")\n",
    "        \n",
    "        # C. Tally the votes for the VERY LAST word\n",
    "        if current_word_idx is not None:\n",
    "            winner_label = 'O'\n",
    "            b_tags = [p for p in current_word_preds if p.startswith('B-')]\n",
    "            if b_tags:\n",
    "                winner_label = b_tags[0]\n",
    "            else:\n",
    "                i_tags = [p for p in current_word_preds if p.startswith('I-')]\n",
    "                if i_tags:\n",
    "                    winner_label = i_tags[0]\n",
    "            \n",
    "            reconciled_preds.append(winner_label)\n",
    "            reconciled_labels.append(current_word_true_label)\n",
    "            print(\"-------------------------------------------------------------------------------------\")\n",
    "            print(f\"TALLY VOTES for Word {current_word_idx} (FINAL): {current_word_preds} -> WINNER: {winner_label} (B-tag priority)\")\n",
    "            print(\"-------------------------------------------------------------------------------------\")\n",
    "\n",
    "        print(\"\\n--- FINAL WORD-LEVEL RECONCILIATION ---\")\n",
    "        print(f\"Reconciled Preds: {reconciled_preds}\")\n",
    "        print(f\"Reconciled Labels: {reconciled_labels}\")\n",
    "\n",
    "# --- Run the trace with the new PRIORITIZED logic ---\n",
    "indices_to_run = [2, 8, 20]\n",
    "\n",
    "trace_prioritized_vote(\n",
    "    trainer=trainer_B, \n",
    "    tokenized_test_dataset=tokenized_datasets_B[\"test\"],\n",
    "    raw_test_dataset=raw_datasets[\"test\"],\n",
    "    indices_to_trace=indices_to_run\n",
    ")\n",
    "# --- Pick 3 examples to trace ---\n",
    "# We pass the DeBERTa trainer, its tokenized test set, and the RAW test set\n",
    "indices_to_run = [2, 5, 20]\n",
    "\n",
    "trace_prioritized_vote(\n",
    "    trainer=trainer_A, \n",
    "    tokenized_test_dataset=tokenized_datasets_A[\"test\"],\n",
    "    raw_test_dataset=raw_datasets[\"test\"],\n",
    "    indices_to_trace=indices_to_run\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "# --- NEW Trace Function (BiLSTM-CRF) ---\n",
    "def trace_bilstm(model_instance, model_name, raw_test_dataset, indices_to_trace):\n",
    "    \n",
    "    # Load the best weights we saved\n",
    "    model_path = f\"{model_name}_best.pth\"\n",
    "    try:\n",
    "        model_instance.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "        model_instance.to(device)\n",
    "        model_instance.eval()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model weights for {model_name}: {e}\")\n",
    "        return\n",
    "\n",
    "    raw_examples_to_trace = raw_test_dataset.select(indices_to_trace)\n",
    "    \n",
    "    print(f\"\\n--- üåä TRACING MODEL: {model_name} ---\")\n",
    "\n",
    "    for i, idx in enumerate(indices_to_trace):\n",
    "        print(f\"\\n--- Example {idx} ---\")\n",
    "        \n",
    "        # 1. Get raw data\n",
    "        original_tokens = raw_examples_to_trace[i]['tokens']\n",
    "        true_label_ids = raw_examples_to_trace[i]['tags']\n",
    "        true_labels_str = [ID2LABEL[str(lid)] for lid in true_label_ids]\n",
    "        \n",
    "        # 2. Convert to Tensors\n",
    "        unk_id = word2id[\"<UNK>\"]\n",
    "        token_ids = [word2id.get(w, unk_id) for w in original_tokens]\n",
    "        \n",
    "        # 3. Create a batch of 1\n",
    "        token_tensor = torch.tensor(token_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        mask_tensor = torch.ones_like(token_tensor, dtype=torch.long).to(device)\n",
    "        \n",
    "        # 4. Get predictions\n",
    "        with torch.no_grad():\n",
    "            emissions = model_instance(token_tensor, mask_tensor)\n",
    "            decoded_ids = model_instance.decode(emissions, mask_tensor)[0]\n",
    "        \n",
    "        pred_labels_str = [ID2LABEL[str(pid)] for pid in decoded_ids]\n",
    "        \n",
    "        # 5. Print the comparison table\n",
    "        print(f\"Original Sentence:\\n{' '.join(original_tokens)}\\n\")\n",
    "        print(\"Word-Level Prediction Trace (Strategy 1: CRF):\")\n",
    "        print(\"-------------------------------------------------------\")\n",
    "        print(f\"{'Word':<20} | {'Predicted Label':<15} | {'True Label':<15}\")\n",
    "        print(\"-------------------------------------------------------\")\n",
    "        \n",
    "        for k in range(len(original_tokens)):\n",
    "            # Add a '‚úÖ' if the prediction is correct\n",
    "            is_correct = pred_labels_str[k] == true_labels_str[k]\n",
    "            mark = \"‚úÖ\" if is_correct else \"‚ùå\"\n",
    "            print(f\"{original_tokens[k]:<20} | {pred_labels_str[k]:<15} | {true_labels_str[k]:<15} | {mark}\")\n",
    "        print(\"-------------------------------------------------------\")\n",
    "\n",
    "\n",
    "# --- Run BiLSTM Traces ---\n",
    "indices_to_run = [2, 5, 20]\n",
    "\n",
    "# Trace Track C1 (BioWordVec)\n",
    "trace_bilstm(\n",
    "    model_instance=model_c1, \n",
    "    model_name=\"Track_C1_BioWordVec\",\n",
    "    raw_test_dataset=raw_datasets[\"test\"],\n",
    "    indices_to_trace=indices_to_run\n",
    ")\n",
    "\n",
    "# Trace Track C2 (FastText)\n",
    "trace_bilstm(\n",
    "    model_instance=model_c2, \n",
    "    model_name=\"Track_C2_FastText\",\n",
    "    raw_test_dataset=raw_datasets[\"test\"],\n",
    "    indices_to_trace=indices_to_run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d96690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db44fda9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
